{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51190415",
   "metadata": {},
   "source": [
    "# Standardizing PGAM fits\n",
    "--------------------------------------\n",
    "\n",
    "In this notebook we propose a pipeline for fitting PGAMs (Balzani et al., 2020, NeurIPS) with a standard input and output format. The procedure has the following advantages:\n",
    "\n",
    "1. It requires minimal coding (formatting the input data in a standard format)\n",
    "2. The output fomat is easily exportable in MATLAB\n",
    "3. The pipeline is compatible with the PGAM docker and singularity images (LINK TO HUBS)\n",
    "4. The pipleine can be easily parallelized for HPC usage (singularity containers)\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "* [Standard input format](#standard-input-format)\n",
    "    * [Neural & behavioral inputs](#inputs)\n",
    "    * [Configuration files](#configuration-files)\n",
    "* [Example with synthetic data](#example-with-synthetic-data)\n",
    "    * [Create and save an example configuration file](#create-and-save-conf)\n",
    "    * [Fittng the model](#fitting-model)\n",
    "        * [List the fits by experiment, condition, neuron, and model configuration](#fit-list)\n",
    "        * [Load inputs, fit and postprocess](#load-fit-save)\n",
    "* [Example with real data](#example-with-real-data)\n",
    "    * [Create and save an example configuration file](#create-and-save-conf-for-real-data)\n",
    "    * [Fittng the model](#fitting-model-for-real-data)\n",
    "        * [List the fits by experiment, condition, neuron, and model configuration](#fit-list-real-data)\n",
    "        * [Load inputs, fit and postprocess](#load-fit-save-real-data)\n",
    "* [Fit via docker](#docker)\n",
    "    * [Mounting volumes](#mount-volumes)\n",
    "    * [Setting paths](#container-path)\n",
    "    * [Fit by running the container](#fit-container)\n",
    "* [HPC & singularity ](#HPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4de83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Standard input format <a name=\"standard-input-format\"></a>\n",
    "\n",
    "In order to fully specify the PGAM model we need:\n",
    "\n",
    "1. A matrix containing the population spike counts (to be used as response variable and/or covariates) (LINK)\n",
    "\n",
    "2. A matrix containing the task variables (additional covariates)(LINK)\n",
    "\n",
    "3. A vector containing trial IDs(LINK)\n",
    "\n",
    "4. A list of covariates to be included and the ID of the unit that we want to fit.(LINK)\n",
    "\n",
    "5. The parameters defining a B-spline for each covariate (usually shared whithin a recording session or an entire experiment). (LINK)\n",
    "    \n",
    "We propose the following standard input format to facilitate model specification, fitting, and saving. \n",
    "\n",
    "### **Neural & behavioral inputs [1-4].** <a name=\"inputs\"></a>\n",
    "\n",
    "Inputs [1-4] will be contained in a single \".npz\" files with keys: \n",
    "\n",
    "- **counts**: numpy.array of dimension (T x N), where T is the total number of time points (bins), and N is the number of neurons. The code has been extensively tested with bins of size 6ms, but works for any size.\n",
    "- **variables**: numpy.array of dimension (T x M), T as above (same bin size), M the number of task variables. For continous variables, each entry is the value taken by the associated variable. For event variables, it is \"1-hot-encoding\": zeros everywhere except for when the event happens, taking a value of '1'. \n",
    "- **trial_id**: numpy.array of dimension T, trial ids associated to each time point. All time points for a given trial take the same value.\n",
    "- **variable_names**: numpy.array of strings of dimension M, name of each covariate in 'variables'.\n",
    "- **neu_names**: numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "- **neu_info**: dict, *optional\n",
    "    neu_info[neu_names[k]]: dict, info about the k-th neuron, $k=0,\\dots,N-1$. keys are the information label.\n",
    "\n",
    "\n",
    "### **Configuration files [5]**<a name=\"configuration-files\"></a>\n",
    "\n",
    "Input [5], the configuration parameters for the B-spline, will be stored with the \"YAML\" file format. Python dict objects can be readily saved in YAML format as follows,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "    import yaml\n",
    "    with open('data.yml', 'w') as outfile:\n",
    "        yaml.dump(python_dictionary, outfile, default_flow_style=False)\n",
    "```\n",
    "\n",
    "where *python_dictionary* is any python dict object whose values are strings, numeric, list containig strings or numeric, or dict.\n",
    "\n",
    "To read a \"YAML\"\n",
    "\n",
    "```python\n",
    "    with open(\"data.yaml\", \"r\") as stream:\n",
    "        try:\n",
    "            python_dicttionary = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "```\n",
    "\n",
    "The dictionary with the \"YAML\" should be structured as follows:\n",
    "```yaml\n",
    "    var_1: # This is an example of a continous variable. \"var_1\" should match an entry in either \"variable_names\". \n",
    "      is_temporal_kernel: false # false for continous variable, true for events (example below)\n",
    "      kernel_direction: .nan # '.nan' when \"is_temporal_kernel: false\". When \"true\", 0 = acausal (bidirectional), '1' = causal (i.e., firing change after the event happens), '-1' = anticipatory (i.e., firing change before event happens).\n",
    "      kernel_length: .nan\n",
    "      order: 4 # Order of the B-spline. A step-function is order = 1. \n",
    "      knots_num: .nan # \".nan\" will use the knots passed (below). An integer will place knots equi-spaced.\n",
    "      knots: # knots for tuning function. The first and last entries are repeated order-times (given recursive nature of B-spline definition).\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -1.6\n",
    "      - -1.2\n",
    "      - -0.7999999999999998\n",
    "      - -0.3999999999999999\n",
    "      - 0.0\n",
    "      - 0.40000000000000036\n",
    "      - 0.8000000000000003\n",
    "      - 1.2000000000000002\n",
    "      - 1.6\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      penalty_type: der # either 'der' or 'diff'. 'der' is derivate-based penality, 'diff' is difference based penality (see Tutorial: XXX) \n",
    "      der: 2 # order of the derivative used for penalization ('2' means second derivative, penalized wiggliness). Default is '2'.\n",
    "      is_cyclic: # note: should be a vector.\n",
    "      - false\n",
    "      lam: 10 # initial lambda for regularization (i.e., how strong is the smoothing penalization. The larger, the more smoothing you get. This is just for initial value, hyper-parameter that is learned in fitting).\n",
    "      samp_period: 0.006 #size of bin for spike counts and behavioral variables (in seconds).\n",
    "    var_2: # This is an example of an event variable (temporal kernel). Name of \"var_2\" has to be in \"variable_names\". \n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 0 # acausal filter (meaning impact of event on firing could happen earlier or after event)\n",
    "      kernel_length: 201 # number of time-points of the total kernel (in this example 201 samples x 6ms). Suggested to be odd number of samples.\n",
    "      order: 4\n",
    "      knots_num: 8 # will create equi-spaces knots.\n",
    "      knots: .nan # '.nan' when 'knots_num' is an integer.\n",
    "      penalty_type: der\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      lam: 10\n",
    "      samp_period: 0.006\n",
    "    neuron_X: # Where \"neuron X\" is an input in \"neu_names\"\n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 1 # must be '1' for auto-correlation.\n",
    "      kernel_length: 201\n",
    "      order: 4\n",
    "      knots_num: 8\n",
    "      knots: .nan\n",
    "      penalty_type: der      \n",
    "      der: 2  \n",
    "      is_cyclic:\n",
    "      - false\n",
    "      lam: 10\n",
    "      samp_period: 0.006\n",
    "    \n",
    "```\n",
    "\n",
    "where ***var_1*** is a prototypical continous (spatial) variable, ***var_2*** is a prototypical event (temporal) variable. ***spike_history*** will be used for the spike-count auto-correlation term (e.g. the neural spike history as its own predictor, as in an auto-regressive model) **and for the neural couplings (e.g. the counts of simultaneously recoded neurons as predictors)**. See the \"PGAM tutorial.ipynb\" for a definition of continous (spatial) and event (temporal) variables, as well as for a description of the B-spline parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27b899",
   "metadata": {},
   "source": [
    "# Example with synthetic data <a name=\"example-with-synthetic-data\"></a>\n",
    "\n",
    "Before diving into the pipeline implementation, we will create an example synthetic dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa90291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('GAM_library/')\n",
    "from GAM_library import *\n",
    "import gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "\n",
    "## inputs parameters\n",
    "num_events = 600\n",
    "time_points = 10 ** 5  # 10 mins at 0.006 second resolution\n",
    "rate = 5. * 0.006  # Hz rate of the final kernel\n",
    "variance = 5.  # variance of input (modeled to be Gaussian)\n",
    "int_knots_num = 20  # num of internal knots for the spline basis\n",
    "order = 4  # spline order\n",
    "\n",
    "## trial_ids: numpy.array of dim T, trial ids associated to each time point, assume 200 trials\n",
    "trial_ids = np.repeat(np.arange(200),time_points//200)\n",
    "\n",
    "## create event (temporal) input\n",
    "idx = np.random.choice(np.arange(time_points), num_events, replace=False)\n",
    "events = np.zeros(time_points)\n",
    "events[idx] = 1\n",
    "\n",
    "## create continous (spatial) variables (spatial_var will drive the neuron, nuisance_var will not drive the neuron)\n",
    "rv = sts.multivariate_normal(mean=[0, 0], cov= variance * np.eye(2))\n",
    "samp = rv.rvs(time_points)\n",
    "spatial_var = samp[:, 0]\n",
    "nuisance_var = samp[:, 1]\n",
    "\n",
    "# truncate inputs to avoid jumps in the resp function\n",
    "sele_idx = np.abs(spatial_var) < 5\n",
    "spatial_var = spatial_var[sele_idx]\n",
    "nuisance_var = nuisance_var[sele_idx]\n",
    "while spatial_var.shape[0] < time_points:\n",
    "    tmpX = rv.rvs(10 ** 4)\n",
    "    sele_idx = np.abs(tmpX[:, 0]) < 5\n",
    "    tmpX = tmpX[sele_idx, :]\n",
    "\n",
    "    spatial_var = np.hstack((spatial_var, tmpX[:, 0]))\n",
    "    nuisance_var = np.hstack((nuisance_var, tmpX[:, 1]))\n",
    "spatial_var = spatial_var[:time_points]\n",
    "nuisance_var = nuisance_var[:time_points]\n",
    "\n",
    "# create a response function (for ground-truth)\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,8),[5]*3))\n",
    "beta = np.arange(10)\n",
    "beta = beta / np.linalg.norm(beta)\n",
    "beta = np.hstack((beta[5:], beta[:5][::-1]))\n",
    "resp_func = lambda x : np.dot(gdh.splineDesign(knots, x, order, der=0),beta)\n",
    "\n",
    "filter_used_conv = sts.gamma.pdf(np.linspace(0,20,100),a=2) - sts.gamma.pdf(np.linspace(0,20,100),a=5)\n",
    "filter_used_conv = np.hstack((np.zeros(101),filter_used_conv))*2\n",
    "\n",
    "# ADD A PIECE OF CODE HERE TO SHOW THE GROUND TRUTH DATA\n",
    "\n",
    "# mean of the spike counts depending on spatial_var and events\n",
    "log_mu0 = resp_func(spatial_var)\n",
    "for tr in np.unique(trial_ids):\n",
    "    log_mu0[trial_ids == tr] = log_mu0[trial_ids == tr] + np.convolve(events[trial_ids == tr], filter_used_conv, mode='same')\n",
    "\n",
    "# adjust mean rate\n",
    "const = np.log(np.mean(np.exp(log_mu0)) / rate)\n",
    "log_mu0 = log_mu0 - const\n",
    "\n",
    "# generate spikes\n",
    "spk_counts = np.random.poisson(np.exp(log_mu0))\n",
    "\n",
    "### save the inputs in the standard formats\n",
    "\n",
    "## counts: numpy.array of dim (T x N), where T is the total number of time points, N is the number of neurons\n",
    "spk_counts = spk_counts.reshape(-1,1) # population of a single neuron\n",
    "\n",
    "## variables: numpy.array of dim (T x M), T as above, M the number of task variables\n",
    "variables = np.zeros((spk_counts.shape[0],3))\n",
    "variables[:, 0] = spatial_var\n",
    "variables[:, 1] = nuisance_var\n",
    "variables[:, 2] = events\n",
    "\n",
    "## variable_names: numpy.array of strings of dimension M, name of each covariate in 'variables'\n",
    "variable_names = ['spatial_1', 'spatial_2', 'events']\n",
    "\n",
    "## neu_names: numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "neu_names = ['neuron_A']\n",
    "\n",
    "## neu_info: dict, *optional neu_info[neu_names[k]]: dict, info about the k-th neuron, 𝑘=0,…,𝑁-1.\n",
    "## keys are the information label.\n",
    "neu_info = {'neuron_A': {'unit_type':'SUA', 'area':'dlPFC'}}\n",
    "\n",
    "np.savez('/notebooks/example_data.npz', counts=spk_counts.reshape(-1,1), variables=variables,variable_names=variable_names,\n",
    "        neu_names=neu_names, neu_info=neu_info, trial_ids=trial_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca293baa",
   "metadata": {},
   "source": [
    "## Create and save an example configuration file <a name=\"create-and-save-conf\"></a>\n",
    "\n",
    "\n",
    "Below the code for an example configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9474439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "# Create a YAML cofiguration file and save\n",
    "# var_1 will be spatial, var_2 will be temporal\n",
    "order = 4\n",
    "knots_1 = np.hstack(([-5]*(order-1), np.linspace(-5,5,15),[5]*(order-1)))\n",
    "knots_2 = np.hstack(([-6]*(order-1), np.linspace(-5,5,15),[6]*(order-1)))\n",
    "\n",
    "\n",
    "# convert to float (instead np.float64, produce a yaml human readable yaml list, optional)\n",
    "knots_1 = [float(k) for k in knots_1]\n",
    "knots_2 = [float(k) for k in knots_2]\n",
    "\n",
    "\n",
    "## change order of this example to match above\n",
    "cov_dict = {\n",
    "    'spatial_1' : {\n",
    "        'lam':10, \n",
    "        'penalty_type': 'der', \n",
    "        'der': 2, \n",
    "        'knots': knots_1,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'spatial_2' : {\n",
    "        'lam':10, \n",
    "        'penalty_type': 'der', \n",
    "        'der': 2, \n",
    "        'knots': knots_2, # need to change this above, in the description of the variables\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'events':\n",
    "    {\n",
    "        'lam':10,\n",
    "        'penalty_type':'der',\n",
    "        'der':2,\n",
    "        'knots': np.nan,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': True,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': 10,\n",
    "        'kernel_length': 500,\n",
    "        'kernel_direction': 0,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'neuron_A':\n",
    "    {\n",
    "        'lam':10,\n",
    "        'penalty_type':'der',\n",
    "        'der':2,\n",
    "        'knots': np.nan,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': True,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': 8,\n",
    "        'kernel_length': 201,\n",
    "        'kernel_direction': 1,\n",
    "        'samp_period':0.006 \n",
    "    }\n",
    "}\n",
    "\n",
    "# save the yaml config\n",
    "with open('/notebooks/config_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(cov_dict, outfile, default_flow_style=False)\n",
    "    \n",
    "\n",
    "# # load the yaml config\n",
    "# with open(\"demo/config_pgam.yml\", \"r\") as stream:\n",
    "#     try:\n",
    "#         cov_dict = yaml.safe_load(stream)\n",
    "#     except yaml.YAMLError as exc:\n",
    "#         print(exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65d903",
   "metadata": {},
   "source": [
    "## Fitting the model <a name=\"fitting-model\"></a>\n",
    "\n",
    "### List the fits by experiment, condition, neuron, and model configuration <a name=\"fit-list\"></a>\n",
    "\n",
    "Once the configuration file has been set-up, we will need a list of fits to be performed. An individual fit is identified by the experiment type, the session, the neuron ID, and the model configuation.\n",
    "\n",
    "When testing different models (including different subset of task variables, changing knots location and density, etc.) one needs to only modify the configuration files, and update the list of fits appropriately. \n",
    "\n",
    "We will again organize the list of fits as a YAML file with the following categories:\n",
    " \n",
    "- **experiment_ID**: string, experiment ID\n",
    "\n",
    "- **session_ID**: string, session ID\n",
    "\n",
    "- **neuron_num**: int, the neuron number (from 0 to N-1, whee N is the number of units in the session)\n",
    "\n",
    "- **path_to_input**: string, the path to the input data\n",
    "\n",
    "- **path_to_config**: string, the path to the configuration file\n",
    "\n",
    "- **path_to_output**: string, the path to the output folder where results will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a2173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save the YAML with the fit list (1 fit only)\n",
    "fit_dict = {\n",
    "    'experiment_ID': ['exp_1'],\n",
    "    'session_ID': ['session_A'],\n",
    "    'neuron_num': [0],\n",
    "    'path_to_input': ['/notebooks/example_data.npz'],\n",
    "    'path_to_config': ['/notebooks/config_example_data.yml'],\n",
    "    'path_to_output': ['/notebooks/']\n",
    "} \n",
    "# save the yaml fit list\n",
    "with open('/notebooks/fit_list_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735465a3",
   "metadata": {},
   "source": [
    "### Load inputs, fit and postprocess <a name=\"load-fit-save\"></a>\n",
    "\n",
    " \n",
    "The code below: loads the input data, the configurations, fits the model, post-process and save the reuslts. The follwing lines of code are also saved as separate script in 'PGAM/utils/fit_from_config.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49559493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT INFO:\n",
      "EXP ID: exp_1\n",
      "SESSION ID: session_A\n",
      "NEURON NUM: 1\n",
      "INPUT DATA PATH: /notebooks/example_data.npz\n",
      "CONFIG PATH: /notebooks/config_example_data.yml\n",
      "\n",
      "\n",
      "adding events...\n",
      "adding neuron_A...\n",
      "adding spatial_1...\n",
      "adding spatial_2...\n",
      "\n",
      "fitting neuron neuron_A...\n",
      "\n",
      "post-process fit results...\n",
      "processing:  events\n",
      "hstack: 0.1054062969997176 sec\n",
      "hstack: 0.028148891000455478 sec\n",
      "processing:  spike_hist\n",
      "processing:  spatial_1\n",
      "processing:  spatial_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import libs\n",
    "import numpy as np\n",
    "import sys,os\n",
    "# append path to GAM_library if not in the envs (not needed if working within a Docker container or \n",
    "# if GAM_library is in the PATH or PYTHONPATH environment variables)\n",
    "sys.path.append('GAM_library/') \n",
    "\n",
    "import GAM_library as gl\n",
    "import gam_data_handlers as gdh\n",
    "from post_processing import postprocess_results\n",
    "import yaml\n",
    "import statsmodels.api as sm\n",
    "from scipy.io import savemat\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "#################################################\n",
    "# User defined input\n",
    "#################################################\n",
    "\n",
    "# frac of the trials used for fit eval\n",
    "frac_eval = 0.2 \n",
    "\n",
    "# PATH to fit list\n",
    "path_fit_list = '/notebooks/fit_list_example_data.yml'\n",
    "\n",
    "# save as mat\n",
    "save_as_mat = True\n",
    "#################################################\n",
    "\n",
    "\n",
    "# load the job id (either as an input form the command line, or as a default value if not passed)\n",
    "argv = sys.argv\n",
    "if len(argv) == 2: # assumes the script is run the command \"python fit_from_config.py fit_num\"\n",
    "    fit_num = int(sys.argv[1]) - 1 # HPC job-array indices starts from 1.\n",
    "else:\n",
    "    fit_num = 0 # set a default value\n",
    "\n",
    "\n",
    "# load fit info\n",
    "with open(path_fit_list, 'r') as stream:\n",
    "    fit_dict = yaml.safe_load(stream)\n",
    "    \n",
    "# unpack the info and load the data\n",
    "experiment_ID = fit_dict['experiment_ID'][fit_num]\n",
    "session_ID = fit_dict['session_ID'][fit_num]\n",
    "neuron_num = fit_dict['neuron_num'][fit_num]\n",
    "path_to_input = fit_dict['path_to_input'][fit_num]\n",
    "path_to_config = fit_dict['path_to_config'][fit_num]\n",
    "path_out = fit_dict['path_to_output'][fit_num]\n",
    "\n",
    "print('FIT INFO:\\nEXP ID: %s\\nSESSION ID: %s\\nNEURON NUM: %d\\nINPUT DATA PATH: %s\\nCONFIG PATH: %s\\n\\n'%(\n",
    "    experiment_ID,session_ID,neuron_num+1,path_to_input,path_to_config))\n",
    "\n",
    "# load & unpack data and config\n",
    "data = np.load(path_to_input, allow_pickle=True)\n",
    "counts = data['counts']\n",
    "variables = data['variables']\n",
    "variable_names = data['variable_names']\n",
    "neu_names = data['neu_names']\n",
    "trial_ids = data['trial_ids']\n",
    "if 'neu_info' in data.keys():\n",
    "    neu_info = data['neu_info'].all()\n",
    "else:\n",
    "    neu_info = {}\n",
    "\n",
    "with open(path_to_config, 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)\n",
    "\n",
    "# create a train and eval set (approximately with the right frac of trials)\n",
    "train_trials = trial_ids % (np.round(1/frac_eval)) != 0\n",
    "eval_trials = ~train_trials\n",
    "\n",
    "# create and populate the smooth handler object\n",
    "sm_handler = gdh.smooths_handler()\n",
    "for var in config_dict.keys():\n",
    "    print('adding %s...'%var)\n",
    "    # check if var is a neuron or a variable\n",
    "    if var in variable_names:\n",
    "        x_var = np.squeeze(variables[:, np.array(variable_names) == var])\n",
    "    elif var in neu_names:\n",
    "        x_var = np.squeeze(counts[:, np.array(neu_names) == var])\n",
    "    else:\n",
    "        raise ValueError('Variable \"%s\" not found in the input data!'%var)\n",
    "    \n",
    "    knots = config_dict[var]['knots']\n",
    "    \n",
    "    if np.isscalar(knots):\n",
    "        knots = None\n",
    "    else:\n",
    "        knots = [np.array(knots)]\n",
    "\n",
    "    lam = config_dict[var]['lam']\n",
    "    penalty_type = config_dict[var]['penalty_type']\n",
    "    der = config_dict[var]['der']\n",
    "    order = config_dict[var]['order']  \n",
    "    is_temporal_kernel = config_dict[var]['is_temporal_kernel']\n",
    "    is_cyclic =  config_dict[var]['is_cyclic']\n",
    "    knots_num = config_dict[var]['knots_num']\n",
    "    kernel_length = config_dict[var]['kernel_length']\n",
    "    kernel_direction = config_dict[var]['kernel_direction']\n",
    "    samp_period = config_dict[var]['samp_period']\n",
    "    \n",
    "    # rename the variable as spike hist if the input is the spike counts of the neuron we are fitting\n",
    "    if var == neu_names[neuron_num]:\n",
    "        label = 'spike_hist'\n",
    "    else:\n",
    "        label = var\n",
    "    \n",
    "    sm_handler.add_smooth(label, [x_var], knots=knots, ord=order, is_temporal_kernel=is_temporal_kernel,\n",
    "                     trial_idx=trial_ids, is_cyclic=is_cyclic, penalty_type=penalty_type, der=der, lam=lam,\n",
    "                         knots_num=knots_num, kernel_length=kernel_length,kernel_direction=kernel_direction,\n",
    "                         time_bin=samp_period)\n",
    "\n",
    "link = sm.genmod.families.links.log()\n",
    "poissFam = sm.genmod.families.family.Poisson(link=link)\n",
    "\n",
    "spk_counts = np.squeeze(counts[:, neuron_num])\n",
    "\n",
    "# create the pgam model\n",
    "pgam = gl.general_additive_model(sm_handler,\n",
    "                              sm_handler.smooths_var, # list of coovarate we want to include in the model\n",
    "                              spk_counts, # vector of spike counts\n",
    "                              poissFam # poisson family with exponential link from statsmodels.api\n",
    "                             )\n",
    "\n",
    "print('\\nfitting neuron %s...\\n'%neu_names[neuron_num])\n",
    "full, reduced = pgam.fit_full_and_reduced(sm_handler.smooths_var, \n",
    "                                          th_pval=0.001,# pval for significance of covariate icluseioon\n",
    "                                          max_iter=10 ** 2, # max number of iteration\n",
    "                                          use_dgcv=True, # learn the smoothing penalties by dgcv\n",
    "                                          trial_num_vec=trial_ids,\n",
    "                                          filter_trials=train_trials)  \n",
    "\n",
    "print('post-process fit results...')\n",
    "res = postprocess_results(neu_names[neuron_num], spk_counts, full, reduced, train_trials,\n",
    "                        sm_handler, poissFam, trial_ids, var_zscore_par=None, info_save=neu_info, bins=100)\n",
    "\n",
    "\n",
    "# saving the file: save_name will be expID_sessionID_neuID_configName\n",
    "\n",
    "config_basename = os.path.basename(path_to_config).split('.')[0]\n",
    "save_name = '%s_%s_%s_%s'%(experiment_ID, session_ID, neu_names[neuron_num], config_basename)\n",
    "\n",
    "if save_as_mat:\n",
    "    savemat(os.path.join(path_out, save_name+'.mat'), mdict={'results':res})\n",
    "else:\n",
    "    np.savez(os.path.join(path_out, save_name+'.npz'), results=res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd1d05",
   "metadata": {},
   "source": [
    "# Fit via docker <a name=\"docker\"></a>\n",
    "\n",
    "\n",
    "Start docker. Make sure that you have the PGAM docker image by listing all available images with the terminal command,\n",
    "```sh\n",
    "    docker images\n",
    "```\n",
    "It should list a repository named edoardobalzani87/pgam, \n",
    "\n",
    "```\n",
    "REPOSITORY              TAG       IMAGE ID       CREATED          SIZE\n",
    "edoardobalzani87/pgam   1.0       052c7daf6ff7   19 minutes ago   4.64GB\n",
    "\n",
    "```\n",
    "\n",
    "### Running the container and mounting volumes <a name=\"mount-volumes\"></a>\n",
    "\n",
    "If you are running the PGAM image in a docker container for fitting the model you must specify which of the host folders (e.g. the folders of the operating system running the container) will be mounted as volumes by the container. This is a necessary step if the container needs to read from or write into the host file system. The syntax of the command is the following,\n",
    "\n",
    "```sh\n",
    "    docker run -v <path to local folder 1>:<path to image folder 1> \\\n",
    "               -v <path to local folder 2>:<path to image folder 2> \\\n",
    "               ...\n",
    "               -ti <docker image to be run> <command> -c \"<additional commands>\"\n",
    "```\n",
    "\n",
    "More specifically, mounting a folder means that: 1) when the container is started, the content of *\\<local folder i>* in the file system of the host will be copied in *\\<image folder i>* of the container temporary file system, 2) whenever the container writes in *\\<image folder i>*  (saving/deleting files or creating/deleting directories), the same will be performed on the host  *\\<local folder i>*.  \n",
    "\n",
    "The docker image has a few folders that one may use to mount local folders:\n",
    "\n",
    "- **/input**: mount this container folder to the host folder containing the input data and the fit list YAML file \n",
    "- **/output**: mount this container folder to the host folder that will contain the output to be saved\n",
    "- **/config**: mount this container folder to the host folder containing the config YAML file\n",
    "- **/scripts**: mount this container folder to the host folder containing the fitting script (fit_from_config.py)\n",
    "\n",
    "### Setting paths <a name=\"container-path\"></a>\n",
    "\n",
    "Make sure to edit the fit list YAML file replacing host folders with container folders, for example:\n",
    "\n",
    "```python\n",
    "fit_dict = {\n",
    "    'experiment_ID': ['exp_1'],\n",
    "    'session_ID': ['session_A'],\n",
    "    'neuron_num': [0],\n",
    "    'path_to_input': ['/input/example_data.npz'],         # '/input/', '/config/', and '/output/' are the path \n",
    "    'path_to_config': ['/config/config_example_data.yml'] # to the folder in the temp file system \n",
    "    'path_to_output': ['/output/']                        # of the container\n",
    "}                                                        \n",
    "# if saving the yaml fit list outsider the docker container\n",
    "with open('<path to local config folder>/fit_list_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "# if saving the yaml fit list inside the docker cointainer\n",
    "#with open('/config/fit_list_example_data.yml', 'w') as outfile:\n",
    "#    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "```\n",
    "Similarly edit the fit script that will load the fit list YAML, pointing the \"config/\" folder of the container. In \"fit_from_config.py\" we would need to set appropriately the variable \"path_fit_list\",\n",
    "\n",
    "```python\n",
    "...\n",
    "#################################################\n",
    "# User defined input\n",
    "#################################################\n",
    "\n",
    "# frac of the trials used for fit eval\n",
    "frac_eval = 0.2 \n",
    "\n",
    "# PATH to fit list\n",
    "path_fit_list = '/input/fit_list_example_data.yml'\n",
    "...\n",
    "```\n",
    "\n",
    "### Fit by running the container <a name=\"fit-container\"></a>\n",
    "\n",
    "Enter the following command in the terminal to create and run the container,\n",
    "\n",
    "```sh\n",
    "docker run -v <path to host input>:/input \\\n",
    "           -v <path to host output>:/output \\\n",
    "           -v <path to host config>:/config \\\n",
    "           -v <path to host scripts>:/scripts \\\n",
    "           -ti edoardobalzani87/pgam:1.0 bin/bash -c \"python /scripts/fit_from_config.py 0\"\n",
    "# note the '0' at the end of the call denotes the first neuron is being fit. Can be looped to fit other neurons in the list\n",
    "```\n",
    "\n",
    "Delete unused containers by listing all the container with the command\n",
    "\n",
    "```sh\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "which will produce something similar to,\n",
    "```sh\n",
    "CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS                     \n",
    "3f3644b47861   edoardobalzani87/pgam:1.0   \"bin/bash -c 'python…\"   About a minute ago   Exited (0)\n",
    "```\n",
    "\n",
    "Reomve the container with,\n",
    "```sh\n",
    "docker rm <CONTAINER ID> \n",
    "```\n",
    "\n",
    "Fit results will be stored in \\<path to host output>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c07382",
   "metadata": {},
   "source": [
    "# HPC & singularity <a name=\"HPC\"></a>\n",
    "\n",
    "This method is currently tested on the NYU green HPC. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbc59ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
