{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4885ade",
   "metadata": {},
   "source": [
    "# Standardizing PGAM fits\n",
    "--------------------------------------\n",
    "\n",
    "In this notebook we propose a pipeline for fitting PGAMs with standard input and output formats. The procedure has the following advantages:\n",
    "\n",
    "1. It requires minimal coding (formatting the input data in a standard format)\n",
    "2. The output fomat is easily exportable in MATLAB\n",
    "3. The pipeline is comppatible with the PGAM docker image\n",
    "4. The pipleine can be easily parallelized for HPC usage (singularity containers)\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "* [Standard input format](#standard-input-format)\n",
    "    * [Neural & behavioral inputs](#inputs)\n",
    "    * [Configuration files](#configuration-files)\n",
    "* [Example with synthetic data](#example-with-synthetic-data)\n",
    "    * [Create and save an example configuration file](#create-and-save-conf)\n",
    "    * [Fittng the model](#fitting-model)\n",
    "        * [List the fits by experiment, condition, neuron, and model configuration](#fit-list)\n",
    "        * [Load inputs, fit and postprocess](#load-fit-save)\n",
    "* [Fit via docker](#docker)\n",
    "    * [Mounting volumes](#mount-volumes)\n",
    "    * [Setting paths](#container-path)\n",
    "    * [Fit by running the container](#fit-container)\n",
    "* [HPC & singularity ](#HPC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4de83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Standard input format <a name=\"standard-input-format\"></a>\n",
    "\n",
    "In order to fully specify the PGAM model we need:\n",
    "\n",
    "1. A matrix containing the population spike counts (to be used as response variable and/or covariatets)\n",
    "\n",
    "2. A matrix containing the task variables (additional covariates)\n",
    "\n",
    "3. A vector containing trial IDs\n",
    "\n",
    "4. A list of covariates to be included and the ID of the unit that we want to fit.\n",
    "\n",
    "5. The parameters defining a B-spline for each covariate (usually shared whithin a recording session or an entire experiment)\n",
    "    \n",
    "We propose the follwing standard input format to facilitate model specification, fitting and saving. \n",
    "\n",
    "### **Neural & behavioral inputs [1-4].** <a name=\"inputs\"></a>\n",
    "\n",
    "Inputs [1-4] will be contained in a single \".npz\" files with keys: \n",
    "\n",
    "- **counts**: numpy.array of dim (T x N), where T is the total number of time points, N is the number of neurons\n",
    "- **variables**: numpy.array of dim (T x M), T as above, M the number of task variables\n",
    "- **trial_id**: numpy.array of dim T, trial ids associated to each time point\n",
    "- **variable_names**: numpy.array of strings of dimension M, name of each covariate in 'variables'\n",
    "- **neu_names**: numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "- **neu_info**: dict, *optional\n",
    "    neu_info[neu_names[k]]: dict, info about the k-th neuron, $k=0,\\dots,N-1$. keys are the information label.\n",
    "\n",
    "\n",
    "### **Configuration files [5]**<a name=\"configuration-files\"></a>\n",
    "\n",
    "Input [5], the configuration parameters for the B-spline, will be stored with the \"YAML\" file format. Python dict objects can be readily saved in JSON format as follows,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "    import yaml\n",
    "    with open('data.yml', 'w') as outfile:\n",
    "        yaml.dump(python_dictionary, outfile, default_flow_style=False)\n",
    "```\n",
    "\n",
    "where *python_dictionary* is any python dict object whose values are strings, numeric, list containig strings or numeric, or dict.\n",
    "\n",
    "To read a \"YAML\"\n",
    "\n",
    "```python\n",
    "    with open(\"data.yaml\", \"r\") as stream:\n",
    "        try:\n",
    "            python_dicttionary = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "```\n",
    "\n",
    "The dictionary with the \"YAML\" will be structured as followa:\n",
    "```yaml\n",
    "    spike_history:\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 1\n",
    "      kernel_length: 201\n",
    "      knots: .nan\n",
    "      knots_num: 8\n",
    "      lam: 10\n",
    "      order: 4\n",
    "      penalty_type: der\n",
    "      samp_period: 0.006\n",
    "    var_1:\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      is_temporal_kernel: false\n",
    "      kernel_direction: .nan\n",
    "      kernel_length: .nan\n",
    "      knots:\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -1.6\n",
    "      - -1.2\n",
    "      - -0.7999999999999998\n",
    "      - -0.3999999999999999\n",
    "      - 0.0\n",
    "      - 0.40000000000000036\n",
    "      - 0.8000000000000003\n",
    "      - 1.2000000000000002\n",
    "      - 1.6\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      knots_num: .nan\n",
    "      lam: 10\n",
    "      order: 4\n",
    "      penalty_type: der\n",
    "      samp_period: 0.006\n",
    "    var_2:\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 0\n",
    "      kernel_length: 201\n",
    "      knots: .nan\n",
    "      knots_num: 8\n",
    "      lam: 10\n",
    "      order: 4\n",
    "      penalty_type: der\n",
    "      samp_period: 0.006\n",
    "```\n",
    "\n",
    "where ***varname_1*** is a prototypical spatial variable, ***varname_2*** is a prototypical temporal variable. ***spike_hist*** will be used for the spike-count auto-correlation term (e.g. the neural spike history as its own predictor, as in an AR model) and for the neural couplings (e.g. the counts of simultaneously recoded neurons as predictors). See the \"PGAM tutorial .ipynb\" for a definition of spatial and temporal variables, and for a description of the B-spline parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d7402",
   "metadata": {},
   "source": [
    "# Example with synthetic data <a name=\"example-with-synthetic-data\"></a>\n",
    "\n",
    "Before divinng into the pipeline implementation, we will create an example synthetic dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2515d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path None do not exist\n",
      "user not uses cuda\n",
      "Module pycuda not found! Use CPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('GAM_library/')\n",
    "from GAM_library import *\n",
    "import gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "\n",
    "## inputs parameters\n",
    "num_events = 6000\n",
    "time_points = 10 ** 5  # 10 mins at 0.006 ms resolution\n",
    "rate = 5. * 0.006  # Hz rate of the final kernel\n",
    "variance = 5.  # spatial input and nuisance variance\n",
    "int_knots_num = 20  # num of internal knots for the spline basis\n",
    "order = 4  # spline order\n",
    "\n",
    "## trial_ids: numpy.array of dim T, trial ids associated to each time point, assume 200 trials\n",
    "trial_ids = np.repeat(np.arange(200),time_points//200)\n",
    "\n",
    "## create temporal input\n",
    "idx = np.random.choice(np.arange(time_points), num_events, replace=False)\n",
    "events = np.zeros(time_points)\n",
    "events[idx] = 1\n",
    "\n",
    "rv = sts.multivariate_normal(mean=[0, 0], cov= variance * np.eye(2))\n",
    "samp = rv.rvs(time_points)\n",
    "spatial_var = samp[:, 0]\n",
    "nuisance_var = samp[:, 1]\n",
    "\n",
    "# truncate X to avoid jumps in the resp function\n",
    "sele_idx = np.abs(spatial_var) < 5\n",
    "spatial_var = spatial_var[sele_idx]\n",
    "nuisance_var = nuisance_var[sele_idx]\n",
    "while spatial_var.shape[0] < time_points:\n",
    "    tmpX = rv.rvs(10 ** 4)\n",
    "    sele_idx = np.abs(tmpX[:, 0]) < 5\n",
    "    tmpX = tmpX[sele_idx, :]\n",
    "\n",
    "    spatial_var = np.hstack((spatial_var, tmpX[:, 0]))\n",
    "    nuisance_var = np.hstack((nuisance_var, tmpX[:, 1]))\n",
    "spatial_var = spatial_var[:time_points]\n",
    "nuisance_var = nuisance_var[:time_points]\n",
    "\n",
    "# create a resp function\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,8),[5]*3))\n",
    "beta = np.arange(10)\n",
    "beta = beta / np.linalg.norm(beta)\n",
    "beta = np.hstack((beta[5:], beta[:5][::-1]))\n",
    "resp_func = lambda x : np.dot(gdh.splineDesign(knots, x, order, der=0),beta)\n",
    "\n",
    "filter_used_conv = sts.gamma.pdf(np.linspace(0,20,100),a=2) - sts.gamma.pdf(np.linspace(0,20,100),a=5)\n",
    "filter_used_conv = np.hstack((np.zeros(101),filter_used_conv))*2\n",
    "# mean of the spike counts depending on spatial_var and events\n",
    "log_mu0 = resp_func(spatial_var)\n",
    "for tr in np.unique(trial_ids):\n",
    "    log_mu0[trial_ids == tr] = log_mu0[trial_ids == tr] + np.convolve(events[trial_ids == tr], filter_used_conv, mode='same')\n",
    "\n",
    "# adjust mean rate\n",
    "const = np.log(np.mean(np.exp(log_mu0)) / rate)\n",
    "log_mu0 = log_mu0 - const\n",
    "\n",
    "# generate spikes\n",
    "spk_counts = np.random.poisson(np.exp(log_mu0))\n",
    "\n",
    "### save the inputs in the standard formats\n",
    "\n",
    "## counts: numpy.array of dim (T x N), where T is the total number of time points, N is the number of neurons\n",
    "spk_counts = spk_counts.reshape(-1,1) # population of a single neuron\n",
    "\n",
    "## variables: numpy.array of dim (T x M), T as above, M the number of task variables\n",
    "variables = np.zeros((spk_counts.shape[0],3))\n",
    "variables[:, 0] = spatial_var\n",
    "variables[:, 1] = nuisance_var\n",
    "variables[:, 2] = events\n",
    "\n",
    "## variable_names: numpy.array of strings of dimension M, name of each covariate in 'variables'\n",
    "variable_names = ['spatial_1', 'spatial_2', 'events']\n",
    "\n",
    "## neu_names: numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "neu_names = ['neuron_A']\n",
    "\n",
    "## neu_info: dict, *optional neu_info[neu_names[k]]: dict, info about the k-th neuron, ð‘˜=0,â€¦,ð‘-1.\n",
    "## keys are the information label.\n",
    "neu_info = {'neuron_A': {'unit_type':'SUA', 'area':'dlPFC'}}\n",
    "\n",
    "np.savez('demo/example_data.npz', counts=spk_counts.reshape(-1,1), variables=variables,variable_names=variable_names,\n",
    "        neu_names=neu_names, neu_info=neu_info, trial_ids=trial_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca293baa",
   "metadata": {},
   "source": [
    "## Create and save an example configuration file <a name=\"create-and-save-conf\"></a>\n",
    "\n",
    "\n",
    "Below the code for an example configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9474439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "# Create a JSON cofiguration file and save\n",
    "# var_1 will be spatial, var_2 will be temporal\n",
    "order = 4\n",
    "knots = np.hstack(([-5]*(order-1), np.linspace(-5,5,15),[5]*(order-1)))\n",
    "\n",
    "\n",
    "# convert to float (instead np.float64, produce a yaml human readable yaml list, optional)\n",
    "knots = [float(k) for k in knots]\n",
    "\n",
    "cov_dict = {\n",
    "    'spatial_1' : {\n",
    "        'lam':10, \n",
    "        'penalty_type': 'der', \n",
    "        'der': 2, \n",
    "        'knots': knots,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'spatial_2' : {\n",
    "        'lam':10, \n",
    "        'penalty_type': 'der', \n",
    "        'der': 2, \n",
    "        'knots': knots,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'events':\n",
    "    {\n",
    "        'lam':10,\n",
    "        'penalty_type':'der',\n",
    "        'der':2,\n",
    "        'knots': np.nan,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': True,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': 10,\n",
    "        'kernel_length': 500,\n",
    "        'kernel_direction': 0,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'neuron_A':\n",
    "    {\n",
    "        'lam':10,\n",
    "        'penalty_type':'der',\n",
    "        'der':2,\n",
    "        'knots': np.nan,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': True,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': 8,\n",
    "        'kernel_length': 201,\n",
    "        'kernel_direction': 1,\n",
    "        'samp_period':0.006 \n",
    "    }\n",
    "}\n",
    "\n",
    "# save the yaml config\n",
    "with open('demo/config_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(cov_dict, outfile, default_flow_style=False)\n",
    "\n",
    "# # load the yaml config\n",
    "# with open(\"demo/config_pgam.yml\", \"r\") as stream:\n",
    "#     try:\n",
    "#         cov_dict = yaml.safe_load(stream)\n",
    "#     except yaml.YAMLError as exc:\n",
    "#         print(exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65d903",
   "metadata": {},
   "source": [
    "## Fittng the model <a name=\"fitting-model\"></a>\n",
    "\n",
    "### List the fits by experiment, condition, neuron, and model configuration <a name=\"fit-list\"></a>\n",
    "\n",
    "Once the configuration file has been set-up, we will need a list of fits to be performed. An individual fit is identified by tthe experiment type, the session, the neuron ID, and the model configuation.\n",
    "\n",
    "When testing different models (including different subset of task variables, changing knots location and density, etc.), one needs to only to modify the configuration files, and update the list of fits appopriately. \n",
    "\n",
    "We will again organize the list of fits as a YAML file with the following cathegories:\n",
    " \n",
    "- **experiment_ID**: string, experiment ID\n",
    "\n",
    "- **session_ID**: string, session ID\n",
    "\n",
    "- **neuron_num**: int, the neuron number (from 0 to N-1, whee N is the number of units in the session)\n",
    "\n",
    "- **path_to_input**: string, the path to the input data\n",
    "\n",
    "- **path_to_config**: string, the path to the configuration file\n",
    "\n",
    "- **path_to_output**: string, the path to the output folder where results will be saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a2173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save the YAML with the fit list (1 fit only)\n",
    "fit_dict = {\n",
    "    'experiment_ID': ['exp_1'],\n",
    "    'session_ID': ['session_A'],\n",
    "    'neuron_num': [0],\n",
    "    'path_to_input': ['demo/example_data.npz'],\n",
    "    'path_to_config': ['demo/config_example_data.yml'],\n",
    "    'path_to_output': ['demo/']\n",
    "} \n",
    "# save the yaml fit list\n",
    "with open('demo/fit_list_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ca79d",
   "metadata": {},
   "source": [
    "### Load inputs, fit and postprocess <a name=\"load-fit-save\"></a>\n",
    "\n",
    " \n",
    "The code below: loads the input data, the configurations, fits the model, post-process and save the reuslts. The follwing lines of code are also saved as separate script in 'PGAM/utils/fit_from_config.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7a2196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT INFO:\n",
      "EXP ID: exp_1\n",
      "SESSION ID: session_A\n",
      "NEURON NUM: 1\n",
      "INPUT DATA PATH: demo/example_data.npz\n",
      "CONFIG PATH: demo/config_example_data.yml\n",
      "\n",
      "\n",
      "adding events...\n",
      "adding neuron_A...\n",
      "adding spatial_1...\n",
      "adding spatial_2...\n",
      "\n",
      "fitting neuron neuron_A...\n",
      "\n",
      "post-process fit results...\n",
      "hstack: 0.16388304500000572 sec\n",
      "hstack: 0.07723113999999498 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import libs\n",
    "import numpy as np\n",
    "import sys,os\n",
    "# apped path to GAM_library if not in the envs (not needed if working within a Docker container or \n",
    "# if GAM_library is in the PATH or PYTHONPATH environment variables)\n",
    "sys.path.append('GAM_library/') \n",
    "\n",
    "import GAM_library as gl\n",
    "import gam_data_handlers as gdh\n",
    "from post_processing import postprocess_results\n",
    "import yaml\n",
    "import statsmodels.api as sm\n",
    "from scipy.io import savemat\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "#################################################\n",
    "# User defined input\n",
    "#################################################\n",
    "\n",
    "# frac of the trials used for fit eval\n",
    "frac_eval = 0.2 \n",
    "\n",
    "# PATH to fit list\n",
    "path_fit_list = 'demo/fit_list_example_data.yml'\n",
    "\n",
    "\n",
    "# save as mat\n",
    "save_as_mat = False\n",
    "#################################################\n",
    "\n",
    "\n",
    "# load the job id (either as an input form the command line, or as a default value if not passed)\n",
    "argv = sys.argv\n",
    "if len(argv) == 2: # assumes the script is run the command \"python fit_from_config.py fit_num\"\n",
    "    fit_num = int(sys.argv[1]) - 1 # HPC job-array indices starts from 1.\n",
    "else:\n",
    "    fit_num = 0 # set a default value\n",
    "\n",
    "\n",
    "# load fit info\n",
    "with open(path_fit_list, 'r') as stream:\n",
    "    fit_dict = yaml.safe_load(stream)\n",
    "    \n",
    "# unpack the info and load the data\n",
    "experiment_ID = fit_dict['experiment_ID'][fit_num]\n",
    "session_ID = fit_dict['session_ID'][fit_num]\n",
    "neuron_num = fit_dict['neuron_num'][fit_num]\n",
    "path_to_input = fit_dict['path_to_input'][fit_num]\n",
    "path_to_config = fit_dict['path_to_config'][fit_num]\n",
    "path_out = fit_dict['path_to_output'][fit_num]\n",
    "\n",
    "print('FIT INFO:\\nEXP ID: %s\\nSESSION ID: %s\\nNEURON NUM: %d\\nINPUT DATA PATH: %s\\nCONFIG PATH: %s\\n\\n'%(\n",
    "    experiment_ID,session_ID,neuron_num+1,path_to_input,path_to_config))\n",
    "\n",
    "# load & unpack data and config\n",
    "data = np.load(path_to_input, allow_pickle=True)\n",
    "counts = data['counts']\n",
    "variables = data['variables']\n",
    "variable_names = data['variable_names']\n",
    "neu_names = data['neu_names']\n",
    "trial_ids = data['trial_ids']\n",
    "if 'neu_info' in data.keys():\n",
    "    neu_info = data['neu_info'].all()\n",
    "else:\n",
    "    neu_info = {}\n",
    "\n",
    "with open(path_to_config, 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)\n",
    "\n",
    "# create a train and eval set (approximately with the right frac of trials)\n",
    "train_trials = trial_ids % (np.round(1/frac_eval)) != 0\n",
    "eval_trials = ~train_trials\n",
    "\n",
    "# create and populate the smooth handler object\n",
    "sm_handler = gdh.smooths_handler()\n",
    "for var in config_dict.keys():\n",
    "    print('adding %s...'%var)\n",
    "    # check if var is a neuron or a variable\n",
    "    if var in variable_names:\n",
    "        x_var = np.squeeze(variables[:, np.array(variable_names) == var])\n",
    "    elif var in neu_names:\n",
    "        x_var = np.squeeze(counts[:, np.array(neu_names) == var])\n",
    "    else:\n",
    "        raise ValueError('Variable \"%s\" not found in the input data!'%var)\n",
    "    \n",
    "    knots = config_dict[var]['knots']\n",
    "    \n",
    "    if np.isscalar(knots):\n",
    "        knots = None\n",
    "    else:\n",
    "        knots = [np.array(knots)]\n",
    "\n",
    "    lam = config_dict[var]['lam']\n",
    "    penalty_type = config_dict[var]['penalty_type']\n",
    "    der = config_dict[var]['der']\n",
    "    order = config_dict[var]['order']  \n",
    "    is_temporal_kernel = config_dict[var]['is_temporal_kernel']\n",
    "    is_cyclic =  config_dict[var]['is_cyclic']\n",
    "    knots_num = config_dict[var]['knots_num']\n",
    "    kernel_length = config_dict[var]['kernel_length']\n",
    "    kernel_direction = config_dict[var]['kernel_direction']\n",
    "    samp_period = config_dict[var]['samp_period']\n",
    "    \n",
    "    # rename the variable as spike hist if the input is the spike counts of the neuron we are fitting\n",
    "    if var == neu_names[neuron_num]:\n",
    "        label = 'spike_hist'\n",
    "    else:\n",
    "        label = var\n",
    "    \n",
    "    sm_handler.add_smooth(label, [x_var], knots=knots, ord=order, is_temporal_kernel=is_temporal_kernel,\n",
    "                     trial_idx=trial_ids, is_cyclic=is_cyclic, penalty_type=penalty_type, der=der, lam=lam,\n",
    "                         knots_num=knots_num, kernel_length=kernel_length,kernel_direction=kernel_direction,\n",
    "                         time_bin=samp_period)\n",
    "\n",
    "link = sm.genmod.families.links.log()\n",
    "poissFam = sm.genmod.families.family.Poisson(link=link)\n",
    "\n",
    "spk_counts = np.squeeze(counts[:, neuron_num])\n",
    "\n",
    "# create the pgam model\n",
    "pgam = gl.general_additive_model(sm_handler,\n",
    "                              sm_handler.smooths_var, # list of coovarate we want to include in the model\n",
    "                              spk_counts, # vector of spike counts\n",
    "                              poissFam # poisson family with exponential link from statsmodels.api\n",
    "                             )\n",
    "\n",
    "print('\\nfitting neuron %s...\\n'%neu_names[neuron_num])\n",
    "full, reduced = pgam.fit_full_and_reduced(sm_handler.smooths_var, \n",
    "                                          th_pval=0.001,# pval for significance of covariate icluseioon\n",
    "                                          max_iter=10 ** 2, # max number of iteration\n",
    "                                          use_dgcv=True, # learn the smoothing penalties by dgcv\n",
    "                                          trial_num_vec=trial_ids,\n",
    "                                          filter_trials=train_trials)  \n",
    "\n",
    "print('post-process fit results...')\n",
    "res = postprocess_results(neu_names[neuron_num], spk_counts, full, reduced, train_trials,\n",
    "                        sm_handler, poissFam, trial_ids, var_zscore_par=None, info_save=neu_info, bins=100)\n",
    "\n",
    "\n",
    "# saving the file: save_name will be expID_sessionID_neuID_configName\n",
    "\n",
    "config_basename = os.path.basename(path_to_config).split('.')[0]\n",
    "save_name = '%s_%s_%s_%s'%(experiment_ID, session_ID, neu_names[neuron_num], config_basename)\n",
    "\n",
    "if save_as_mat:\n",
    "    savemat(os.path.join(path_out, save_name+'.mat'), mdict={'results':res})\n",
    "else:\n",
    "    np.savez(os.path.join(path_out, save_name+'.npz'), results=res)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb436cf",
   "metadata": {},
   "source": [
    "# Fit via docker <a name=\"docker\"></a>\n",
    "\n",
    "\n",
    "Start docker. Make sure that you have the PGAM docker image by listing all available imagegs with the terminal command,\n",
    "```sh\n",
    "    docker images\n",
    "```\n",
    "It should list a repository named edoardobalzani87/pgam, \n",
    "\n",
    "```\n",
    "REPOSITORY              TAG       IMAGE ID       CREATED          SIZE\n",
    "edoardobalzani87/pgam   1.0       052c7daf6ff7   19 minutes ago   4.64GB\n",
    "\n",
    "```\n",
    "\n",
    "### Running the container and mounting volumes <a name=\"mount-volumes\"></a>\n",
    "\n",
    "If you are running the PGAM image in a docker container for fitting the model you must specify which of the host folders (e.g. the folders of the operating system running the container) will be mounted as volumes by the container. This is a necessary step if the container needs to read from or write into the host file system. The syntax of the command is the following,\n",
    "\n",
    "```sh\n",
    "    docker run -v <path to local folder 1>:<path to image folder 1> \\\n",
    "               -v <path to local folder 2>:<path to image folder 2> \\\n",
    "               ...\n",
    "               -ti <docker image to be run> <command> -c \"<additional commands>\"\n",
    "```\n",
    "\n",
    "More specifically, mounting a folder means that: 1) when the container is started, the content of *\\<local folder i>* in the file system of the host will be copied in *\\<image folder i>* of the container temporary file system, 2) whenever the container writes in *\\<image folder i>*  (saving/deleting files or cerating/deleting directories), the same will be performed on the host  *\\<local folder i>*.  \n",
    "\n",
    "The docker image has a few folders that one may use to mount local folders:\n",
    "\n",
    "- **/input**: mount this contaier folder the host folder containing the input data and the fit list YAML file \n",
    "- **/output**: mount this contaier folder the host folder that will contain the output to be saved\n",
    "- **/config**: mount this contaier folder the host folder containing the config YAML file\n",
    "- **/scripts**: mount this contaier folder the host folder containing fitting script (fit_from_config.py)\n",
    "\n",
    "### Setting paths <a name=\"container-path\"></a>\n",
    "\n",
    "Make sure to edit the fit list YAML file replacing host folders with container folders, for example:\n",
    "\n",
    "```python\n",
    "fit_dict = {\n",
    "    'experiment_ID': ['exp_1'],\n",
    "    'session_ID': ['session_A'],\n",
    "    'neuron_num': [0],\n",
    "    'path_to_input': ['input/example_data.npz'],         # 'input/', 'config/', and 'output/' are the path \n",
    "    'path_to_config': ['config/config_example_data.yml'] # to the folder in the temp file system \n",
    "    'path_to_output': ['output/']                        # of the container\n",
    "}                                                        \n",
    "# save the yaml fit list\n",
    "with open('<path to local config folder>/fit_list_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "\n",
    "```\n",
    "\n",
    "Similarly edit the fit script that will load the fit list YAML, pointing the \"config/\" folder of the container. In \"fit_from_config.py\" we would need to set appropriately the variable \"path_fit_list\",\n",
    "\n",
    "```python\n",
    "...\n",
    "#################################################\n",
    "# User defined input\n",
    "#################################################\n",
    "\n",
    "# frac of the trials used for fit eval\n",
    "frac_eval = 0.2 \n",
    "\n",
    "# PATH to fit list\n",
    "path_fit_list = 'input/fit_list_example_data.yml'\n",
    "...\n",
    "```\n",
    "\n",
    "### Fit by running the container <a name=\"fit-container\"></a>\n",
    "\n",
    "Enter the following command in the terminal to create and run the container,\n",
    "\n",
    "```sh\n",
    "docker run -v <path to host input>:/input \\\n",
    "           -v <path to host output>:/output \\\n",
    "           -v <path to host config>:/config \\\n",
    "           -v <path to host scripts>:/scripts \\\n",
    "           -ti edoardobalzani87/pgam:1.0 bin/bash -c \"python scripts/fit_from_config.py 0\"\n",
    "\n",
    "```\n",
    "\n",
    "Delete delete unused containers by listing all the container with the command\n",
    "\n",
    "```sh\n",
    "docker ps -a\n",
    "```\n",
    "\n",
    "which will produce something similar to,\n",
    "```sh\n",
    "CONTAINER ID   IMAGE                       COMMAND                  CREATED              STATUS                     \n",
    "3f3644b47861   edoardobalzani87/pgam:1.0   \"bin/bash -c 'pythonâ€¦\"   About a minute ago   Exited (0)\n",
    "```\n",
    "\n",
    "Reomve the container with,\n",
    "```sh\n",
    "docker rm <CONTAINER ID> \n",
    "```\n",
    "\n",
    "Fit results will be stored in \\<path to host output>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9f7a9",
   "metadata": {},
   "source": [
    "# HPC & singularity <a name=\"HPC\"></a>\n",
    "\n",
    "This method is currently tested on the NYU green HPC. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d033736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
