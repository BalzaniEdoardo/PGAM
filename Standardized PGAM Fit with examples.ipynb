{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ae51bb",
   "metadata": {},
   "source": [
    "# Standardizing PGAM fits\n",
    "--------------------------------------\n",
    "\n",
    "In this notebook we propose a pipeline for fitting PGAMs with standard input and output formats. The procedure has the following advantages:\n",
    "\n",
    "1. It requires minimal coding (formatting the input data in a standard format)\n",
    "2. It is easy to parallelizable (for HPC usage, for example)\n",
    "3. The output fomat is easily exportable in MATLAB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4de83",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## Standard input format\n",
    "\n",
    "In order to fully specify the PGAM model we need:\n",
    "\n",
    "1. A matrix containing the population spike counts (to be used as response variable and/or covariatets)\n",
    "\n",
    "2. A matrix containing the task variables (additional covariates)\n",
    "\n",
    "3. A vector containing trial IDs\n",
    "\n",
    "4. A list of covariates to be included and the ID of the unit that we want to fit.\n",
    "\n",
    "5. The parameters defining a B-spline for each covariate (usually shared whithin a recording session or an entire experiment)\n",
    "    \n",
    "We propose the follwing standard input format to facilitate model specification, fitting and saving. \n",
    "\n",
    "### **Neural & behavioral data [1-4].** \n",
    "\n",
    "Inputs [1-4] will be contained in a single \".npz\" files with keys: \n",
    "\n",
    "- **counts**: numpy.array of dim (T x N), where T is the total number of time points, N is the number of neurons\n",
    "- **variables**: numpy.array of dim (T x M), T as above, M the number of task variables\n",
    "- **trial_id**: numpy.array of dim T, trial ids associated to each time point\n",
    "- **variable_names**: numpy.array of strings of dimension M, name of each covariate in 'variables'\n",
    "- **neu_names**: numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "- **neu_info**: dict, *optional\n",
    "    neu_info[neu_names[k]]: dict, info about the k-th neuron, $k=0,\\dots,N-1$. keys are the information label.\n",
    "\n",
    "\n",
    "### **Configuration [5]**\n",
    "\n",
    "Input [5], the configuration parameters for the B-spline, will be stored with the \"YAML\" file format. Python dict objects can be readily saved in JSON format as follows,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "    import yaml\n",
    "    with open('data.yml', 'w') as outfile:\n",
    "        yaml.dump(python_dictionary, outfile, default_flow_style=False)\n",
    "```\n",
    "\n",
    "where *python_dictionary* is any python dict object whose values are strings, numeric, list containig strings or numeric, or dict.\n",
    "\n",
    "To read a \"YAML\"\n",
    "\n",
    "```python\n",
    "    with open(\"data.yaml\", \"r\") as stream:\n",
    "        try:\n",
    "            python_dicttionary = yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "```\n",
    "\n",
    "The dictionary with the \"YAML\" will be structured as followa:\n",
    "```yaml\n",
    "    spike_history:\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 1\n",
    "      kernel_length: 201\n",
    "      knots: .nan\n",
    "      knots_num: 8\n",
    "      lam: 10\n",
    "      order: 4\n",
    "      penalty_type: der\n",
    "      samp_period: 0.006\n",
    "    var_1:\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      is_temporal_kernel: false\n",
    "      kernel_direction: .nan\n",
    "      kernel_length: .nan\n",
    "      knots:\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -2.0\n",
    "      - -1.6\n",
    "      - -1.2\n",
    "      - -0.7999999999999998\n",
    "      - -0.3999999999999999\n",
    "      - 0.0\n",
    "      - 0.40000000000000036\n",
    "      - 0.8000000000000003\n",
    "      - 1.2000000000000002\n",
    "      - 1.6\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      - 2.0\n",
    "      knots_num: .nan\n",
    "      lam: 10\n",
    "      order: 4\n",
    "      penalty_type: der\n",
    "      samp_period: 0.006\n",
    "    var_2:\n",
    "      der: 2\n",
    "      is_cyclic:\n",
    "      - false\n",
    "      is_temporal_kernel: true\n",
    "      kernel_direction: 0\n",
    "      kernel_length: 201\n",
    "      knots: .nan\n",
    "      knots_num: 8\n",
    "      lam: 10\n",
    "      order: 4\n",
    "      penalty_type: der\n",
    "      samp_period: 0.006\n",
    "```\n",
    "\n",
    "where ***varname_1*** is a prototypical spatial variable, ***varname_2*** is a prototypical temporal variable. ***spike_hist*** will be used for the spike-count auto-correlation term (e.g. the neural spike history as its own predictor, as in an AR model) and for the neural couplings (e.g. the counts of simultaneously recoded neurons as predictors). See the \"PGAM tutorial .ipynb\" for a definition of spatial and temporal variables, and for a description of the B-spline parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9bd3e",
   "metadata": {},
   "source": [
    "## Create synthetic data\n",
    "\n",
    "Before divinng into the pipeline implementation, we will create an example synthetic dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d07b2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path None do not exist\n",
      "user not uses cuda\n",
      "Module pycuda not found! Use CPU\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('GAM_library/')\n",
    "from GAM_library import *\n",
    "import gam_data_handlers as gdh\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "from post_processing import postprocess_results\n",
    "\n",
    "## inputs parameters\n",
    "num_events = 6000\n",
    "time_points = 10 ** 5  # 10 mins at 0.006 ms resolution\n",
    "rate = 5. * 0.006  # Hz rate of the final kernel\n",
    "variance = 5.  # spatial input and nuisance variance\n",
    "int_knots_num = 20  # num of internal knots for the spline basis\n",
    "order = 4  # spline order\n",
    "\n",
    "## trial_ids: numpy.array of dim T, trial ids associated to each time point, assume 200 trials\n",
    "trial_ids = np.repeat(np.arange(200),time_points//200)\n",
    "\n",
    "## create temporal input\n",
    "idx = np.random.choice(np.arange(time_points), num_events, replace=False)\n",
    "events = np.zeros(time_points)\n",
    "events[idx] = 1\n",
    "\n",
    "rv = sts.multivariate_normal(mean=[0, 0], cov= variance * np.eye(2))\n",
    "samp = rv.rvs(time_points)\n",
    "spatial_var = samp[:, 0]\n",
    "nuisance_var = samp[:, 1]\n",
    "\n",
    "# truncate X to avoid jumps in the resp function\n",
    "sele_idx = np.abs(spatial_var) < 5\n",
    "spatial_var = spatial_var[sele_idx]\n",
    "nuisance_var = nuisance_var[sele_idx]\n",
    "while spatial_var.shape[0] < time_points:\n",
    "    tmpX = rv.rvs(10 ** 4)\n",
    "    sele_idx = np.abs(tmpX[:, 0]) < 5\n",
    "    tmpX = tmpX[sele_idx, :]\n",
    "\n",
    "    spatial_var = np.hstack((spatial_var, tmpX[:, 0]))\n",
    "    nuisance_var = np.hstack((nuisance_var, tmpX[:, 1]))\n",
    "spatial_var = spatial_var[:time_points]\n",
    "nuisance_var = nuisance_var[:time_points]\n",
    "\n",
    "# create a resp function\n",
    "knots = np.hstack(([-5]*3, np.linspace(-5,5,8),[5]*3))\n",
    "beta = np.arange(10)\n",
    "beta = beta / np.linalg.norm(beta)\n",
    "beta = np.hstack((beta[5:], beta[:5][::-1]))\n",
    "resp_func = lambda x : np.dot(gdh.splineDesign(knots, x, order, der=0),beta)\n",
    "\n",
    "filter_used_conv = sts.gamma.pdf(np.linspace(0,20,100),a=2) - sts.gamma.pdf(np.linspace(0,20,100),a=5)\n",
    "filter_used_conv = np.hstack((np.zeros(101),filter_used_conv))*2\n",
    "# mean of the spike counts depending on spatial_var and events\n",
    "log_mu0 = resp_func(spatial_var)\n",
    "for tr in np.unique(trial_ids):\n",
    "    log_mu0[trial_ids == tr] = log_mu0[trial_ids == tr] + np.convolve(events[trial_ids == tr], filter_used_conv, mode='same')\n",
    "\n",
    "# adjust mean rate\n",
    "const = np.log(np.mean(np.exp(log_mu0)) / rate)\n",
    "log_mu0 = log_mu0 - const\n",
    "\n",
    "# generate spikes\n",
    "spk_counts = np.random.poisson(np.exp(log_mu0))\n",
    "\n",
    "### save the inputs in the standard formats\n",
    "\n",
    "## counts: numpy.array of dim (T x N), where T is the total number of time points, N is the number of neurons\n",
    "spk_counts = spk_counts.reshape(-1,1) # population of a single neuron\n",
    "\n",
    "## variables: numpy.array of dim (T x M), T as above, M the number of task variables\n",
    "variables = np.zeros((spk_counts.shape[0],3))\n",
    "variables[:, 0] = spatial_var\n",
    "variables[:, 1] = nuisance_var\n",
    "variables[:, 2] = events\n",
    "\n",
    "## variable_names: numpy.array of strings of dimension M, name of each covariate in 'variables'\n",
    "variable_names = ['spatial_1', 'spatial_2', 'events']\n",
    "\n",
    "## neu_names: numpy.array of strings of dimension N, label uniquely identifying the neurons in 'counts'\n",
    "neu_names = ['neuron_A']\n",
    "\n",
    "## neu_info: dict, *optional neu_info[neu_names[k]]: dict, info about the k-th neuron, 𝑘=0,…,𝑁-1.\n",
    "## keys are the information label.\n",
    "neu_info = {'neuron_A': {'unit_type':'SUA', 'area':'dlPFC'}}\n",
    "\n",
    "np.savez('demo/example_data.npz', counts=spk_counts.reshape(-1,1), variables=variables,variable_names=variable_names,\n",
    "        neu_names=neu_names, neu_info=neu_info, trial_ids=trial_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca293baa",
   "metadata": {},
   "source": [
    "## Create and save an example configuration file\n",
    "\n",
    "\n",
    "Below the code for an example configuration file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9474439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "# Create a JSON cofiguration file and save\n",
    "# var_1 will be spatial, var_2 will be temporal\n",
    "order = 4\n",
    "knots = np.hstack(([-5]*(order-1), np.linspace(-5,5,15),[5]*(order-1)))\n",
    "\n",
    "\n",
    "# convert to float (instead np.float64, produce a yaml human readable yaml list, optional)\n",
    "knots = [float(k) for k in knots]\n",
    "\n",
    "cov_dict = {\n",
    "    'spatial_1' : {\n",
    "        'lam':10, \n",
    "        'penalty_type': 'der', \n",
    "        'der': 2, \n",
    "        'knots': knots,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'spatial_2' : {\n",
    "        'lam':10, \n",
    "        'penalty_type': 'der', \n",
    "        'der': 2, \n",
    "        'knots': knots,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': False,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': np.nan,\n",
    "        'kernel_length': np.nan,\n",
    "        'kernel_direction': np.nan,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'events':\n",
    "    {\n",
    "        'lam':10,\n",
    "        'penalty_type':'der',\n",
    "        'der':2,\n",
    "        'knots': np.nan,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': True,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': 10,\n",
    "        'kernel_length': 500,\n",
    "        'kernel_direction': 0,\n",
    "        'samp_period':0.006 \n",
    "    },\n",
    "    'neuron_A':\n",
    "    {\n",
    "        'lam':10,\n",
    "        'penalty_type':'der',\n",
    "        'der':2,\n",
    "        'knots': np.nan,\n",
    "        'order':order,\n",
    "        'is_temporal_kernel': True,\n",
    "        'is_cyclic': [False],\n",
    "        'knots_num': 8,\n",
    "        'kernel_length': 201,\n",
    "        'kernel_direction': 1,\n",
    "        'samp_period':0.006 \n",
    "    }\n",
    "}\n",
    "\n",
    "# save the yaml config\n",
    "with open('demo/config_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(cov_dict, outfile, default_flow_style=False)\n",
    "\n",
    "# # load the yaml config\n",
    "# with open(\"demo/config_pgam.yml\", \"r\") as stream:\n",
    "#     try:\n",
    "#         cov_dict = yaml.safe_load(stream)\n",
    "#     except yaml.YAMLError as exc:\n",
    "#         print(exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65d903",
   "metadata": {},
   "source": [
    "## Fittng the model\n",
    "\n",
    "### List the experiments, sessions, neurons and configurations\n",
    "\n",
    "Once the configuration file has been set-up, we will need a list of fits to be performed. An individual fit is identified by tthe experiment type, the session, the neuron ID, and the model configuation.\n",
    "\n",
    "When testing different models (including different subset of task variables, changing knots location and density, etc.), one needs to only to modify the configuration files, and update the list of fits appopriately. \n",
    "\n",
    "We will again organize the list of fits as a YAML file with the following cathegories:\n",
    " \n",
    "- **experiment_ID**: string, experiment ID\n",
    "\n",
    "- **session_ID**: string, session ID\n",
    "\n",
    "- **neuron_num**: int, the neuron number (from 0 to N-1, whee N is the number of units in the session)\n",
    "\n",
    "- **path_to_input**: string, the path to the input data\n",
    "\n",
    "- **path_to_config**: string, the path to the configuration file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28a2173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save the YAML with the fit list (1 fit only)\n",
    "fit_dict = {\n",
    "    'experiment_ID': ['exp_1'],\n",
    "    'session_ID': ['session_A'],\n",
    "    'neuron_num': [0],\n",
    "    'path_to_input': ['demo/example_data.npz'],\n",
    "    'path_to_config': ['demo/config_example_data.yml']\n",
    "} \n",
    "# save the yaml fit list\n",
    "with open('demo/fit_list_example_data.yml', 'w') as outfile:\n",
    "    yaml.dump(fit_dict, outfile, default_flow_style=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1851dd",
   "metadata": {},
   "source": [
    "## Load inputs, fit and postprocess\n",
    "\n",
    "The code below: loads the input data, the configurations, fits the model, post-process and save the reuslts. The follwing lines of code are also saved as separate script in 'PGAM/utils/fit_from_config.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80108663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT INFO:\n",
      "EXP ID: exp_1\n",
      "SESSION ID: session_A\n",
      "NEURON NUM: 1\n",
      "INPUT DATA PATH: demo/example_data.npz\n",
      "CONFIG PATH: demo/config_example_data.yml\n",
      "\n",
      "\n",
      "adding events...\n",
      "adding neuron_A...\n",
      "adding spatial_1...\n",
      "adding spatial_2...\n",
      "\n",
      "fitting neuron neuron_A...\n",
      "\n",
      "post-process fit results...\n",
      "hstack: 0.16388304500000572 sec\n",
      "hstack: 0.07723113999999498 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import libs\n",
    "import numpy as np\n",
    "import sys,os\n",
    "# apped path to GAM_library if not in the envs (not needed if working within a Docker container or \n",
    "# if GAM_library is in the PATH or PYTHONPATH environment variables)\n",
    "sys.path.append('GAM_library/') \n",
    "\n",
    "import GAM_library as gl\n",
    "import gam_data_handlers as gdh\n",
    "from post_processing import postprocess_results\n",
    "import yaml\n",
    "import statsmodels.api as sm\n",
    "from scipy.io import savemat\n",
    "\n",
    "np.random.seed(4)\n",
    "\n",
    "#################################################\n",
    "# User defined input\n",
    "#################################################\n",
    "\n",
    "# frac of the trials used for fit eval\n",
    "frac_eval = 0.2 \n",
    "\n",
    "# PATH to fit list\n",
    "path_fit_list = 'demo/fit_list_example_data.yml'\n",
    "\n",
    "# path to output folder\n",
    "path_out = 'demo/'\n",
    "\n",
    "# save as mat\n",
    "save_as_mat = False\n",
    "#################################################\n",
    "\n",
    "\n",
    "# load the job id (either as an input form the command line, or as a default value if not passed)\n",
    "argv = sys.argv\n",
    "if len(argv) == 2: # assumes the script is run the command \"python fit_from_config.py fit_num\"\n",
    "    fit_num = int(sys.argv[1]) - 1 # HPC job-array indices starts from 1.\n",
    "else:\n",
    "    fit_num = 0 # set a default value\n",
    "\n",
    "\n",
    "# load fit info\n",
    "with open(path_fit_list, 'r') as stream:\n",
    "    fit_dict = yaml.safe_load(stream)\n",
    "    \n",
    "# unpack the info and load the data\n",
    "experiment_ID = fit_dict['experiment_ID'][fit_num]\n",
    "session_ID = fit_dict['session_ID'][fit_num]\n",
    "neuron_num = fit_dict['neuron_num'][fit_num]\n",
    "path_to_input = fit_dict['path_to_input'][fit_num]\n",
    "path_to_config = fit_dict['path_to_config'][fit_num]\n",
    "\n",
    "print('FIT INFO:\\nEXP ID: %s\\nSESSION ID: %s\\nNEURON NUM: %d\\nINPUT DATA PATH: %s\\nCONFIG PATH: %s\\n\\n'%(\n",
    "    experiment_ID,session_ID,neuron_num+1,path_to_input,path_to_config))\n",
    "\n",
    "# load & unpack data and config\n",
    "data = np.load(path_to_input, allow_pickle=True)\n",
    "counts = data['counts']\n",
    "variables = data['variables']\n",
    "variable_names = data['variable_names']\n",
    "neu_names = data['neu_names']\n",
    "trial_ids = data['trial_ids']\n",
    "if 'neu_info' in data.keys():\n",
    "    neu_info = data['neu_info'].all()\n",
    "else:\n",
    "    neu_info = {}\n",
    "\n",
    "with open(path_to_config, 'r') as stream:\n",
    "    config_dict = yaml.safe_load(stream)\n",
    "\n",
    "# create a train and eval set (approximately with the right frac of trials)\n",
    "train_trials = trial_ids % (np.round(1/frac_eval)) != 0\n",
    "eval_trials = ~train_trials\n",
    "\n",
    "# create and populate the smooth handler object\n",
    "sm_handler = gdh.smooths_handler()\n",
    "for var in config_dict.keys():\n",
    "    print('adding %s...'%var)\n",
    "    # check if var is a neuron or a variable\n",
    "    if var in variable_names:\n",
    "        x_var = np.squeeze(variables[:, np.array(variable_names) == var])\n",
    "    elif var in neu_names:\n",
    "        x_var = np.squeeze(counts[:, np.array(neu_names) == var])\n",
    "    else:\n",
    "        raise ValueError('Variable \"%s\" not found in the input data!'%var)\n",
    "    \n",
    "    knots = config_dict[var]['knots']\n",
    "    \n",
    "    if np.isscalar(knots):\n",
    "        knots = None\n",
    "    else:\n",
    "        knots = [np.array(knots)]\n",
    "\n",
    "    lam = config_dict[var]['lam']\n",
    "    penalty_type = config_dict[var]['penalty_type']\n",
    "    der = config_dict[var]['der']\n",
    "    order = config_dict[var]['order']  \n",
    "    is_temporal_kernel = config_dict[var]['is_temporal_kernel']\n",
    "    is_cyclic =  config_dict[var]['is_cyclic']\n",
    "    knots_num = config_dict[var]['knots_num']\n",
    "    kernel_length = config_dict[var]['kernel_length']\n",
    "    kernel_direction = config_dict[var]['kernel_direction']\n",
    "    samp_period = config_dict[var]['samp_period']\n",
    "    \n",
    "    # rename the variable as spike hist if the input is the spike counts of the neuron we are fitting\n",
    "    if var == neu_names[neuron_num]:\n",
    "        label = 'spike_hist'\n",
    "    else:\n",
    "        label = var\n",
    "    \n",
    "    sm_handler.add_smooth(label, [x_var], knots=knots, ord=order, is_temporal_kernel=is_temporal_kernel,\n",
    "                     trial_idx=trial_ids, is_cyclic=is_cyclic, penalty_type=penalty_type, der=der, lam=lam,\n",
    "                         knots_num=knots_num, kernel_length=kernel_length,kernel_direction=kernel_direction,\n",
    "                         time_bin=samp_period)\n",
    "\n",
    "link = sm.genmod.families.links.log()\n",
    "poissFam = sm.genmod.families.family.Poisson(link=link)\n",
    "\n",
    "spk_counts = np.squeeze(counts[:, neuron_num])\n",
    "\n",
    "# create the pgam model\n",
    "pgam = gl.general_additive_model(sm_handler,\n",
    "                              sm_handler.smooths_var, # list of coovarate we want to include in the model\n",
    "                              spk_counts, # vector of spike counts\n",
    "                              poissFam # poisson family with exponential link from statsmodels.api\n",
    "                             )\n",
    "\n",
    "print('\\nfitting neuron %s...\\n'%neu_names[neuron_num])\n",
    "full, reduced = pgam.fit_full_and_reduced(sm_handler.smooths_var, \n",
    "                                          th_pval=0.001,# pval for significance of covariate icluseioon\n",
    "                                          max_iter=10 ** 2, # max number of iteration\n",
    "                                          use_dgcv=True, # learn the smoothing penalties by dgcv\n",
    "                                          trial_num_vec=trial_ids,\n",
    "                                          filter_trials=train_trials)  \n",
    "\n",
    "print('post-process fit results...')\n",
    "res = postprocess_results(neu_names[neuron_num], spk_counts, full, reduced, train_trials,\n",
    "                        sm_handler, poissFam, trial_ids, var_zscore_par=None, info_save=neu_info, bins=100)\n",
    "\n",
    "\n",
    "# saving the file: save_name will be expID_sessionID_neuID_configName\n",
    "\n",
    "config_basename = os.path.basename(path_to_config).split('.')[0]\n",
    "save_name = '%s_%s_%s_%s'%(experiment_ID, session_ID, neu_names[neuron_num], config_basename)\n",
    "\n",
    "if save_as_mat:\n",
    "    savemat(os.path.join(path_out, save_name+'.mat'), mdict={'results':res})\n",
    "else:\n",
    "    np.savez(os.path.join(path_out, save_name+'.npz'), results=res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d07a0b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_example_data.yml\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "config_basename = os.path.basename(path_to_config)\n",
    "print(config_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36455d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
