import numpy as np
import sys,os,re
folder_name = os.path.dirname(os.path.realpath(__file__))
main_dir = os.path.dirname(folder_name)
sys.path.append(os.path.join(main_dir,'GAM_library'))
sys.path.append(os.path.join(main_dir,'firefly_utils'))
sys.path.append(os.path.join(folder_name,'util_preproc'))
from path_class import get_paths_class
from scipy.io import loadmat,savemat
from data_handler import *
#from extract_presence_rate import *

user_paths = get_paths_class()

use_server = None#'/Volumes/server2/Monkeys'
# =============================================================================
# Here you should give the base directory that contains the .mat
# the code will walk through alll subdirectory, and if it will find a
# mat file with the correct name it will list it as a file to be concatenated
#
# =============================================================================

min_bin_size = 0.01 # min size of a spike count bin in sec


DIRECT = '/Volumes/WD_Edo/firefly_analysis/LFP_band/DATASET_accel/'
sv_folder = '/Volumes/WD_Edo/firefly_analysis/LFP_band/concatenation_with_accel'

if not os.path.exists(DIRECT):
    DIRECT = '/scratch/jpn5/gpfa_v0203/dataFF/'
    sv_folder = '/scratch/jpn5/gpfa_v0203/dataFF/'

# folder in which you'll save the output files

print('The code assumes that the lfp_session.mat  files are in the same folder as the session.mat file!')
# list of session to be concatenated


concat_list = []
fld_list = []
pattern_fh = '^m\d+s\d+.mat$'
minSess = 12
for root, dirs, files in os.walk(DIRECT, topdown=False):
    for name in files:
        if re.match(pattern_fh,name):
            if not 'm72' in name:
                continue
            sess_num = int(name.split('s')[1].split('.')[0])
            if sess_num < minSess:
                continue
            concat_list += [name.split('.mat')[0]]
            fld_list += [root]

concat_list = ['m53s113']

ptrn = '^m\d+s\d+.mat$'
ptrn = '^m\d+s\d+$'



fld_list = [DIRECT]*len(concat_list)

save = True
send = True

base_file = DIRECT

baseflld = os.path.dirname(base_file)

# list of session in which forcing the use of left eye posiiton
use_left_eye = ['53s48']


# keys in the mat file generated by the preprocessing script of  K.
behav_stat_key = 'behv_stats'
spike_key = 'units'
behav_dat_key = 'trials_behv'
lfp_key = 'lfps'





# presence rate params
occupancy_bin_sec = 60 # at least one spike per min
occupancy_rate_th = 0.1 # hz

linearprobe_sampling_fq = 20000
utah_array_sampling_fq = 30000

phase_precomputed = []
cnt_concat = 0


for session in concat_list:
    

    if session in use_left_eye:
        use_eye = 'left'
    else:
        use_eye = 'right'

    if session in ['m51s43','m51s38','m51s41','m51s42','m51s40']:
        fhLFP = '/Volumes/WD_Edo/firefly_analysis/LFP_band/DATASET_accel/lfps_%s.mat'%session
    else:
        fhLFP = ''
    base_file = fld_list[cnt_concat]
    cnt_concat += 1

    

    print('loading session %s...'%session)
    pre_trial_dur = 0.2
    post_trial_dur = 0.2

    try:
        dat = loadmat(os.path.join(base_file,'%s.mat'%(session)))
    except:
        print('could not find', session)
        continue
    try:
        lfp_beta = loadmat(os.path.join(base_file,'lfp_beta_%s.mat'%session))
        lfp_alpha = loadmat(os.path.join(base_file,'lfp_alpha_%s.mat'%session))
        lfp_theta = loadmat(os.path.join(base_file,'lfp_theta_%s.mat'%session))
    except:
        print('could not find LFP', session)
        continue 
        
    

    if 'is_phase' in lfp_beta.keys():
        is_phase = lfp_beta['is_phase'][0,0]
    else:
        is_phase = False
    
    if is_phase:
        phase_precomputed += [session]
        continue

    exp_data = data_handler(dat, behav_dat_key, spike_key, lfp_key, behav_stat_key, pre_trial_dur=pre_trial_dur,
                        post_trial_dur=post_trial_dur,
                        lfp_beta=lfp_beta['lfp_beta'], lfp_alpha=lfp_alpha['lfp_alpha'],lfp_theta=lfp_theta['lfp_theta'], extract_lfp_phase=(not is_phase),
                        use_eye=use_eye,fhLFP=fhLFP)
    

    exp_data.set_filters('all', True)
    # impose all replay trials
    exp_data.filter = exp_data.filter + exp_data.info.get_replay(0,skip_not_ok=False)

    t_targ = dict_to_vec(exp_data.behav.events.t_targ)
    t_move = dict_to_vec(exp_data.behav.events.t_move)

    t_start = np.min(np.vstack((t_move, t_targ)), axis=0) - pre_trial_dur
    t_stop = dict_to_vec(exp_data.behav.events.t_end) + post_trial_dur



    var_names = ('rad_vel','ang_vel','rad_path','ang_path','rad_target','ang_target',
                 'lfp_beta','lfp_alpha','lfp_theta','t_move','t_flyOFF','t_stop','t_reward','eye_vert','eye_hori',
                 'hand_vel1','hand_vel2','rad_acc','ang_acc')

    time_pts, rate = exp_data.GPFA_YU_preprocessing([('t_targ','t_targ_off',15),('t_targ_off','t_stop',50),('t_stop','t_reward',15)])

    trial_id = np.arange(rate.shape[0])

    select = exp_data.filter & ((np.diff(time_pts,axis=1) > min_bin_size).prod(axis=1) > 0)

    trial_id = trial_id[select]
    rate = rate[select]
    time_pts = time_pts[select]
    print('SAVING')
    savemat(os.path.join(DIRECT,'test_%s_gpfa.mat'%session), mdict={'dat': {'spikes':rate,'timeDiscr':time_pts,'trialId':trial_id}})
