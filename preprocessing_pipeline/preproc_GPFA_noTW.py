import numpy as np
import sys,os,re
folder_name = os.path.dirname(os.path.realpath(__file__))
main_dir = os.path.dirname(folder_name)
sys.path.append(os.path.join(main_dir,'GAM_library'))
sys.path.append(os.path.join(main_dir,'firefly_utils'))
sys.path.append(os.path.join(folder_name,'util_preproc'))
from path_class import get_paths_class
from scipy.io import loadmat,savemat
from data_handler import *
#from extract_presence_rate import *


def time_stamps_rebin(time_stamps, binwidth_ms=20):
    rebin = {}
    for tr in time_stamps.keys():
        ts = time_stamps[tr]
        tp_num = np.floor((ts[-1] - ts[0]) * 1000 / (binwidth_ms))
        rebin[tr] = ts[0] + np.arange(tp_num) * binwidth_ms / 1000.
    return rebin

user_paths = get_paths_class()

use_server = None#'/Volumes/server2/Monkeys'
# =============================================================================
# Here you should give the base directory that contains the .mat
# the code will walk through alll subdirectory, and if it will find a
# mat file with the correct name it will list it as a file to be concatenated
#
# =============================================================================

min_bin_size = 0.01 # min size of a spike count bin in sec


DIRECT = '/Volumes/WD_Edo/firefly_analysis/LFP_band/DATASET_accel/'
sv_folder = '/Volumes/WD_Edo/firefly_analysis/LFP_band/concatenation_with_accel'

if not os.path.exists(DIRECT):
    DIRECT = '/scratch/jpn5/gpfa_v0203/dataFF/'
    sv_folder = '/scratch/jpn5/gpfa_v0203/dataFF/'

# folder in which you'll save the output files

print('The code assumes that the lfp_session.mat  files are in the same folder as the session.mat file!')
# list of session to be concatenated


concat_list = []
fld_list = []
pattern_fh = '^m\d+s\d+.mat$'
minSess = 12
for root, dirs, files in os.walk(DIRECT, topdown=False):
    for name in files:
        if re.match(pattern_fh,name):
            if not 'm72' in name:
                continue
            sess_num = int(name.split('s')[1].split('.')[0])
            if sess_num < minSess:
                continue
            concat_list += [name.split('.mat')[0]]
            fld_list += [root]

concat_list = ['m53s113']

ptrn = '^m\d+s\d+.mat$'
ptrn = '^m\d+s\d+$'



fld_list = [DIRECT]*len(concat_list)

save = True
send = True

base_file = DIRECT

baseflld = os.path.dirname(base_file)

# list of session in which forcing the use of left eye posiiton
use_left_eye = ['53s48']


# keys in the mat file generated by the preprocessing script of  K.
behav_stat_key = 'behv_stats'
spike_key = 'units'
behav_dat_key = 'trials_behv'
lfp_key = 'lfps'





# presence rate params
occupancy_bin_sec = 60 # at least one spike per min
occupancy_rate_th = 0.1 # hz

linearprobe_sampling_fq = 20000
utah_array_sampling_fq = 30000

phase_precomputed = []
cnt_concat = 0


for session in concat_list:
    

    if session in use_left_eye:
        use_eye = 'left'
    else:
        use_eye = 'right'

    if session in ['m51s43','m51s38','m51s41','m51s42','m51s40']:
        fhLFP = '/Volumes/WD_Edo/firefly_analysis/LFP_band/DATASET_accel/lfps_%s.mat'%session
    else:
        fhLFP = ''
    base_file = fld_list[cnt_concat]
    cnt_concat += 1

    

    print('loading session %s...'%session)
    pre_trial_dur = 0.2
    post_trial_dur = 0.2

    try:
        dat = loadmat(os.path.join(base_file,'%s.mat'%(session)))
    except:
        print('could not find', session)
        continue
    try:
        lfp_beta = loadmat(os.path.join(base_file,'lfp_beta_%s.mat'%session))
        lfp_alpha = loadmat(os.path.join(base_file,'lfp_alpha_%s.mat'%session))
        lfp_theta = loadmat(os.path.join(base_file,'lfp_theta_%s.mat'%session))
    except:
        print('could not find LFP', session)
        continue 
        
    

    if 'is_phase' in lfp_beta.keys():
        is_phase = lfp_beta['is_phase'][0,0]
    else:
        is_phase = False
    
    if is_phase:
        phase_precomputed += [session]
        continue

    exp_data = data_handler(dat, behav_dat_key, spike_key, lfp_key, behav_stat_key, pre_trial_dur=pre_trial_dur,
                        post_trial_dur=post_trial_dur,
                        lfp_beta=lfp_beta['lfp_beta'], lfp_alpha=lfp_alpha['lfp_alpha'],lfp_theta=lfp_theta['lfp_theta'], extract_lfp_phase=(not is_phase),
                        use_eye=use_eye,fhLFP=fhLFP,extract_fly_and_monkey_xy=True)
    

    exp_data.set_filters('all', True)
    # impose all replay trials
    exp_data.filter = exp_data.filter + exp_data.info.get_replay(0,skip_not_ok=False)

    t_targ = dict_to_vec(exp_data.behav.events.t_targ)
    t_move = dict_to_vec(exp_data.behav.events.t_move)

    t_start = np.min(np.vstack((t_move, t_targ)), axis=0) - pre_trial_dur
    t_stop = dict_to_vec(exp_data.behav.events.t_end) + post_trial_dur

    bin_ts = time_stamps_rebin(exp_data.behav.time_stamps, binwidth_ms=20)
    exp_data.spikes.bin_spikes(bin_ts, t_start=t_start, t_stop=t_stop, select=exp_data.filter)

    var_names = ('rad_vel', 'ang_vel', 'rad_path', 'ang_path', 'rad_target', 'ang_target',
                 'eye_vert','eye_hori',
                 'rad_acc','ang_acc')

    time_pts, rate, sm_traj, raw_traj, fly_pos, cov_dict = exp_data.GPFA_YU_preprocessing_noTW( t_start, t_stop, var_list=var_names,binwidth_ms=20)



    rates_str = {}
    time_pts_dict = {}
    trialId = {}

    for kk in rate.keys():
        time_pts_dict['tr_%d'%kk] = time_pts[kk]
        rates_str['tr_%d'%kk] = rate[kk]
        trialId['tr_%d' % kk] = kk
    print('SAVING')
    savemat(os.path.join(DIRECT,'noTW_%s_gpfa.mat'%session), mdict={'dat': {'spikes':rates_str,'timeDiscr':time_pts_dict,'trialId':trialId},
                                                                    'sm_trajectory':sm_traj,'raw_trajectory':raw_traj,
                                                                    'fly_pos':fly_pos,'var_struct':cov_dict,'info_trial':exp_data.info.trial_type})



